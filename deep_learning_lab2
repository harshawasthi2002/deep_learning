{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harshawasthi3167/deep-learning-lab2?scriptVersionId=190789532\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the activation functions\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x > 0, x, x * alpha)\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / exp_x.sum(axis=0)\n\n# Generate input data\nx = np.linspace(-10, 10, 400)\n\n# Calculate output for each activation function\nsigmoid_output = sigmoid(x)\ntanh_output = tanh(x)\nrelu_output = relu(x)\nleaky_relu_output = leaky_relu(x)\nsoftmax_output = softmax(x)\n\n# Plot the activation functions\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 3, 1)\nplt.plot(x, sigmoid_output, label='Sigmoid')\nplt.title('Sigmoid')\nplt.grid(True)\n\nplt.subplot(2, 3, 2)\nplt.plot(x, tanh_output, label='Tanh', color='orange')\nplt.title('Tanh')\nplt.grid(True)\n\nplt.subplot(2, 3, 3)\nplt.plot(x, relu_output, label='ReLU', color='green')\nplt.title('ReLU')\nplt.grid(True)\n\nplt.subplot(2, 3, 4)\nplt.plot(x, leaky_relu_output, label='Leaky ReLU', color='red')\nplt.title('Leaky ReLU')\nplt.grid(True)\n\nplt.subplot(2, 3, 5)\nplt.plot(x, softmax_output, label='Softmax', color='purple')\nplt.title('Softmax')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Array creation\na = np.array([1, 3, 5, 7])\nb = np.array([1, 3, 5, 7])  # Changed to match the shape of 'a'\nc = np.linspace(-10, 10, 5)\nd = np.array([[1, 2, 3], [4, 5, 6]])\nprint(f\"d.ndim: {d.ndim}\")\n\ne = np.zeros(4)\ne = np.ones(4)\nf = np.zeros((3, 4))\ng = np.eye(3)\n\n# Random number generation\nh = np.random.rand()  # Random number between 0 and 1\nprint(f\"Random number between 0 and 1: {h}\")\n\nh = np.random.randn()  # Random number from standard normal distribution\nprint(f\"Random number from standard normal distribution: {h}\")\n\nh = np.random.random()  # Float between 0.0 and 1.0\nprint(f\"Random float between 0.0 and 1.0: {h}\")\n\nh = np.random.randint(4, 20, 5)  # Random integers in the given range\nprint(f\"5 random integers between 4 and 19: {h}\")\n\ni = np.random.rand(2, 5)  # 2x5 matrix with random numbers between 0 and 1\nprint(f\"2x5 matrix with random numbers between 0 and 1:\\n{i}\")\n\nprint(\"\\n===========\")\nprint(\"Operators\")\nprint(\"===========\")\n\n# Operators (a and b now have compatible shapes)\nprint(f\"a + b: {np.add(a, b)}\")\nprint(f\"a - b: {np.subtract(a, b)}\")\nprint(f\"a * b: {np.multiply(a, b)}\")\nprint(f\"a / b: {np.divide(a, b)}\")\nprint(f\"a % b: {np.mod(a, b)}\")\nprint(f\"a ** b: {np.power(a, b)}\")\nprint(f\"dot(a, b): {np.dot(a, b)}\")\n\n# Other operations\nx = np.array([1, 2, 3, 4, 5])\nprint(f\"\\nOperations on x = {x}\")\nprint(f\"max(x): {np.max(x)}\")\nprint(f\"min(x): {np.min(x)}\")\nprint(f\"sqrt(x): {np.sqrt(x)}\")\nprint(f\"square(x): {np.square(x)}\")\nprint(f\"sin(x): {np.sin(x)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Training data\ntrain_inputs = np.array([0, 1, 2, 3, 4])\ntrain_outputs = np.array([0, 6, 12, 18, 24])\n\n# Training loop\nfor i in range(3):\n    weight = int(input(\"Enter the Value of Weight: \"))\n    pred_output = train_inputs * weight\n    error = pred_output - train_outputs\n    \n    print(f\"Weight: {weight}\")\n    print(\"Predicted Output:\", pred_output)\n    print(\"Desired Output:\", train_outputs)\n    print(\"Errors:\", error)\n    print()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nD = np.array([[1,2,3],  # dataset\n              [1,0,2],\n              [0,1,4],\n              [2,1,4]])\n\n# Initialize weight matrix\nnp.random.seed(1)\nw = np.random.random((3,1))\nprint(\"Weight Matrix : \")\nprint(w)\n\n#Forward Pass\nfor iteration in range(1):\n    iLayer = D\n    oPer = np.dot(iLayer,w)     # Perceptron\n    oLayer = 1/(1+np.exp(-oPer))  # Sigmoid\n\nprint(\"Input : \")\nprint(D)\nprint(\"Predicted Output: \")\nprint(oLayer)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nD = np.array([[1,2,3],  # dataset\n              [1,0,2],\n              [0,1,4],\n              [2,1,4]])\n\n# Initialize weight matrix\nnp.random.seed(1)\nw = np.random.random((3,2))\nprint(\"Weight Matrix : \")\nprint(w)\n\n#Forward Pass\nfor iteration in range(1):\n    iLayer = D\n    oPer = np.dot(iLayer,w)\n    oLayer = 1/(1+np.exp(-oPer))\n\nprint(\"Input : \")\nprint(D)\nprint(\"Predicted Output: \")\nprint(oLayer)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nD = np.array([[1,2,3],\n              [1,0,2],\n              [0,1,4],\n              [2,1,4]])\n\n# Initialize Weight Matrices\nnp.random.seed(1)\nW = np.random.random((3,4))\nprint(\"W weight matrix:\")\nprint(W)\n\nprint(\"V weight matrix:\")\nV = np.random.random((4,2))\nprint(V)\n\nfor iteration in range(1):\n    iLayer = D\n    \n    hP = np.dot(iLayer,W)        # Hidden Layer\n    hLayer = 1/(1+np.exp(-hP))\n    \n    oP = np.dot(hLayer,V)        # Output Layer\n    oLayer = 1/(1+np.exp(-oP))\n\nprint(\"Input :\")\nprint(D)  # Changed from 'training' to 'D'\nprint(\"Predicted Output: \")\nprint(oLayer)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}